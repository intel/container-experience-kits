---
###########################
## Profile Configuration ##
###########################
## Do not modify values listed here
## Re-run the "make" command to change profile configuration

profile_name: {{ name }}
configured_arch: {{ arch }}
configured_nic: {{ nic }}

########################
## Host Configuration ##
########################

{% if sriov_operator in ['on', 'optional'] or sriov_network_dp in ['on', 'optional'] or qat in ['on', 'optional'] or dsa in ['on', 'optional'] or fpga in ['on', 'optional'] %}
# Enable IOMMU (required for SR-IOV networking and QAT)
iommu_enabled: {% if (sriov_operator == 'on' or sriov_network_dp == 'on' or qat == 'on' or dsa == 'on' or dlb == 'on' or fpga == 'on') %}true{% else %}false{% endif %}


{% endif %}
{% if hugepages in ['on', 'optional'] %}
# Enables hugepages support
hugepages_enabled: {% if hugepages == 'on' %}true{% else %}false{% endif %}

# Hugepage sizes available: 2M, 1G
default_hugepage_size: {% if vpp == 'on' %}2M{% else %}1G{% endif %}

# Sets how many hugepages should be created
number_of_hugepages_1G: 4
number_of_hugepages_2M: 1024
{% endif %}

{% if isolcpu in ["on", "optional"] %}
# CPU isolation from Linux scheduler
isolcpus_enabled: {% if isolcpu == 'on' %}true{% else %}false{% endif %}

{% if on_vms == 'on' %}
isolcpus: "4-15"
{% else %}
{% if vm_mode == 'on' %}
# isolcpus variable can't be enabled in case of VMRA deployment.
# Its content is generated automatically.
# isolcpus: ""
{% else %}
isolcpus: "4-11"
{% endif %}
{% endif %}
{% endif %}

{% if cpusets in ["on", "optional"] %}
# CPU shielding
cpusets_enabled: {% if cpusets == 'on' %}true{% else %}false{% endif %}

{% if on_vms == 'on' %}
cpusets: "4-15"
{% else %}
cpusets: "4-11"
{% endif %}
{% endif %}

{% if dpdk in ['on', 'optional'] %}
# Install DPDK (required for SR-IOV networking)
install_dpdk: {% if dpdk == 'on' %}true{% else %}false{% endif %}

# DPDK version (will be in action if install_dpdk: true)
dpdk_version: {% if (intel_flexran == 'on' or ovs_dpdk == 'on' or infra_power_manager == 'on') %}"22.11.1"{% elif imtl == 'on'%}"23.03"{% else %}"23.11"{% endif %}

# Custom DPDK patches local path
{% if intel_flexran == 'on' %}dpdk_local_patches_dir: "/tmp/flexran"{% else %}#dpdk_local_patches_dir: "/tmp/patches/dpdk"{% endif %}

# It might be necessary to adjust the patch strip parameter, update as required.
{% if intel_flexran == 'on' %}dpdk_local_patches_strip: 1{% else %}#dpdk_local_patches_strip: 0{% endif %}

{% endif %}

{% if openssl in ['on', 'optional'] %}
# Install and configure OpenSSL cryptography
openssl_install: {% if openssl == 'on' %}true{% else %}false{% endif %}

{% endif %}
# Useful if system loses IP after reboot. Note: make sure IP is stable / system gets same IP after reboot else will cause failures.
# It is needed only in some lab environments. Default setting is false.
# TODO: JP: This workaround should not be needed any longer. To be removed completely once it pass complete validation cycle
enable_dhclient_systemd_service: false

##################################
## Network Device Configuration ##
##################################

{% if on_vms != 'on' %}
default_ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.45.0.pkg"{% else %}gtp.pkgo{% endif %}

# default driver to use with dataplane_interfaces auto-configuration
dataplane_interface_default_vf_driver: "iavf"
{% endif %}

# dataplane interface configuration list
# leave empty for auto-configuration of NIC dataplane interfaces.
dataplane_interfaces: []
#dataplane_interfaces:
{% if on_vms == 'on' %}
#  - bus_info: "06:00.0"            # PCI bus info
#    pf_driver: iavf                # Driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
#  - bus_info: "07:00.0"
#    pf_driver: iavf
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "08:00.0"
#    pf_driver: iavf
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "09:00.0"
#    pf_driver: iavf
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
{% else %}
#  - bus_info: "18:00.0"                    # PCI bus info
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}                         # PF driver, "i40e", "ice"
{% if ddp_legacy in ['on', 'optional'] or intel_ethernet_operator.ddp_update in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.45.0.pkg"{% else %}gtp.pkgo{% endif %}  # DDP package name to be loaded into the NIC
                                            # For i40e(XV710-*) allowable ddp values are: "ecpri.pkg", "esp-ah.pkg", "ppp-oe-ol2tpv2.pkgo", "mplsogreudp.pkg" and "gtp.pkgo", replace as required
                                            # For ice(E810-*) allowable ddp values are: ice_comms-1.3.[17,20,22,24,28,30,31,35,37,40,45].0.pkg  such as "ice_comms-1.3.45.0.pkg", replace as required
                                            # ddp_profile must be defined for first port of each network device. bifurcated cards will appear as unique devices.
{% endif %}
{% if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}              # This option is for Intel E810 Series NICs and requires Intel Ethernet Operator and Flow Config to be enabled in group vars.
                                            # With Flow Configuration enabled the first VF (VF0) will be reserved for Flow Configuration and the rest of VFs will be indexed starting from 1.
{% endif %}
{% if intel_flexran in ['on', 'optional'] %}
#    default_vf_driver: "vfio-pci"          # FlexRAN in POD requires SRIOV VFs with vfio-pci
#    sriov_numvfs: 4                        # Total number of VFs for FlexRAN in POD must be 4
{% else %}
#    default_vf_driver: "iavf"              # Default driver to be used with VFs if specific driver is not defined in the "sriov_vfs" section
#    sriov_numvfs: 8                        # Total number of VFs to create including VFs listed in the "sriov_vfs" section.
                                            # If total number of VFs listed in the "sriov_vfs" section is greater than "sriov_numvfs" then excessive entities will be ignored.
                                            # VF's name should follow scheme: <arbitrary_vf_name>_<zero_started_index_of_vf>
                                            # If index in the VF's name is greater than "sriov_numfs - 1" such VF will be ignored.
{% endif %}
{% if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
{% if intel_flexran not in ['on', 'optional'] %}
#    sriov_vfs:                             # List of VFs to create on this PF with specific driver
#      vf_00: "vfio-pci"                    # VF driver to be attached to this VF under this PF. Options: "iavf", "vfio-pci", "igb_uio"
#      vf_05: "vfio-pci"
{% endif %}

#  - bus_info: "18:00.1"
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}

{% if ddp_legacy in ['on', 'optional'] or intel_ethernet_operator.ddp_update in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.45.0.pkg"{% else %}gtp.pkgo{% endif %}

{% endif %}
{% if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}

{% endif %}
#    default_vf_driver: "vfio-pci"
#    sriov_numvfs: 4
{% if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
{% if intel_flexran not in ['on', 'optional'] %}
#    sriov_vfs: {}                          # No VFs with specific driver on this PF or "sriov_vfs" can be omitted for convenience
{% endif %}
{% endif %}

{% if nic_drivers in ['on', 'optional'] %}
# Set 'true' to update / downgrade i40e, ice and iavf kernel modules
update_nic_drivers: {% if nic_drivers == 'on' %}true{% else %}false{% endif %}

# The below options can be used to downgrade drivers. This is not recommended and users should proceed at their own risk.
#i40e_driver_version: "2.23.17"
#i40e_driver_checksum: "sha1:57904a541212174c20bacfbf109401c19c59c25c"
#ice_driver_version: "1.12.7"
#ice_driver_checksum: "sha1:b286f3bdf48c2a355f6c1b0ed2a3d82bce21c6df"
#iavf_driver_version: "4.8.2"
#iavf_driver_checksum: "sha1:fcc997aebeee3744e621e0fd3290205bd18f6a45"

{% endif %}
# Set 'true' to upgrade / downgrade NIC firmware. This will be executed on all NICs listed in "dataplane_interfaces[*].bus_info".
# Downgrading firmware is not recommended and users should proceed at their own risk.
update_nic_firmware: false
{% if nic == 'fvl' %}
#nvmupdate:
#  i40e:
#    nvmupdate_pkg_url: "https://downloadmirror.intel.com/786060/700Series_NVMUpdatePackage_v9_30_Linux.tar.gz"
#    nvmupdate_pkg_checksum: "sha1:929987FAB30394C86AA1BBADDAA62BB7D4E8CC0E"
#    required_fw_version: "9.30"
#    # min fw version for ddp was taken from:
#    # https://www.intel.com/content/www/us/en/developer/articles/technical/dynamic-device-personalization-for-intel-ethernet-700-series.html
#    min_ddp_loadable_fw_version: "6.01"
#    min_updatable_fw_version: "5.02"
#    # when downgrading only, the recommended below version is required to download the supported NVMupdate64E tool. Users should replace the tool at their own risk.
#    supported_nvmupdate_tool_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#    supported_nvmupdate_tool_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#    supported_nvmupdate_tool_fw_version: "4.0"

{% endif %}
{% if nic == 'cvl' %}
#nvmupdate:
#  ice:
#    nvmupdate_pkg_url: "https://downloadmirror.intel.com/769278/E810_NVMUpdatePackage_v4_20_Linux.tar.gz"
#    nvmupdate_pkg_checksum: "sha1:36CE159E53E6060F2AC4E3419DB8A21E3D982A85"
#    required_fw_version: "4.20"
#    # https://builders.intel.com/docs/networkbuilders/intel-ethernet-controller-800-series-device-personalization-ddp-for-telecommunications-workloads-technology-guide.pdf
#    # document above does not specify any min fw version needed for ddp feature. So, min_ddp_loadable_fw is the same as min_updatable_fw
#    min_ddp_loadable_fw_version: "0.70"
#    min_updatable_fw_version: "0.70"
#    # when downgrading only, the recommended below version is required to download the supported NVMupdate64E tool. Users should replace the tool at their own risk.
#    supported_nvmupdate_tool_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#    supported_nvmupdate_tool_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#    supported_nvmupdate_tool_fw_version: "4.0"

{% endif %}
{% if ddp_legacy in ['on', 'optional'] %}
# install Intel x700 & x800 series NICs DDP packages (Legacy)
install_ddp_packages: {% if ddp_legacy == 'on' and nic == 'fvl'%}true{% else %}false{% endif %}

# If following error appears: "Flashing failed: Operation not permitted"
# run deployment with update_nic_firmware: true
# or Disable DDP installation via install_ddp_packages: false

{% endif %}
{% if ddp_legacy in ['on', 'optional'] or intel_ethernet_operator.ddp_update | default("") in ['on', 'optional'] %}
enable_ice_systemd_service: {% if ddp_legacy == "on" or intel_ethernet_operator.ddp_update | default("") == "on" %}true{% else %}false{% endif %}  # Enable custom ddp package to be loaded after reboot

{% endif %}
#######################
## CNI Configuration ##
#######################

{% if sriov_network_dp in ['on', 'optional'] %}
sriov_cni_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if bond_cni in ['on', 'optional'] %}
# Bond CNI
bond_cni_enabled: {% if bond_cni == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if network_userspace in ['on', 'optional'] %}
# Userspace CNI related configuration, applied when userspace_cni_enabled set to 'true' in group_vars
userspace_cni:
  vswitch: {% if vpp == 'on' %}vpp{% else %}ovs{% endif %} # Supported values: ovs, vpp
  # OVS DPDK related configuration
  ovs_version: "v3.2.1" # OVS version has to be compatible/functional with the DPDK version set by 'dpdk_version'
  ovs_dpdk_lcore_mask: 0x1 # CPU mask for OVS-DPDK PMD threads
  ovs_dpdk_socket_mem: "256,0" # Hugepages allocated by OVS-DPDK per NUMA node in megabytes
                               # Example 1: "256,512" allocates 256MB from node 0 and 512MB from node 1
                               # Example 2: "1024" allocates 1GB from node 0 on a single socket board, e.g. in a VM

{% endif %}
##################
## CPU Features ##
##################

{% if native_cpu_manager in ["on", "optional"] %}
# Native CPU Manager (Kubernetes built-in)
# These settings are relevant only if in group_vars native_cpu_manager_enabled: true
native_cpu_manager_system_reserved_cpus: 2000m   # Amount of CPU cores reserved for the housekeeping (2000m = 2000 millicores = 2 cores)
#native_cpu_manager_kube_reserved_cpus: 1000m     # BUG NPF-8495 - uncommenting causes deployment to fail; Amount of CPU cores reserved for Kubelet
#native_cpu_manager_reserved_cpus: "0,1,2"        # Explicit list of the CPUs reserved for the host level system threads and Kubernetes related threads
                                                  # Note: All remaining unreserved CPU cores will be consumed by the workloads.

{% endif %}
######################
## Storage Features ##
######################

# When group_vars: storage_deploy_test_mode == true, RA will create loop device for storage, no need to configure the list.
# otherwise, an actual NVME or SSD device for storage on the device name.

{% if minio in ['on', 'optional'] or lpvsp in ['on', 'optional'] or rook_ceph in ['on', 'optional'] %}
# node storage configuration
persistent_volumes: []
#persistent_volumes:
#  - name: "mnt-data-1"                         # PV identifier will be used for PVs names followed by node name(e.g., mnt-data-1-hostname)
{% if minio in ['on', 'optional'] %}
#    storageClassName: "local-storage"          # Storage class name to match with PVC
#    accessMode: "ReadWriteOnce"                # Access mode when mounting a volume, e.g., ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
#    persistentVolumeReclaimPolicy: "Retain"    # Reclaim policy when a volume is released once it's bound, e.g., Retain/Recycle/Delete
{% endif %}
#    mountPath: /mnt/disks/disk1                # Mount path of a volume, for local provisioner, it musts match /mnt/disks/* pattern
#    device: /dev/nvme0n1                       # Target storage device name when creating a volume. Only set it when storage_deploy_test_mode is false
#    fsType: ext4                               # file system types, [ext4, xfs]

#  - name: "mnt-data-2"
{% if minio in ['on', 'optional'] %}
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
{% endif %}
#    mountPath: /mnt/disks/disk2
#    device: /dev/nvme1n1
#    fsType: ext4

#  - name: "mnt-data-3"
{% if minio in ['on', 'optional'] %}
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
{% endif %}
#    mountPath: /mnt/disks/disk3
#    device: /dev/nvme2n1
#    fsType: ext4

#  - name: "mnt-data-4"
{% if minio in ['on', 'optional'] %}
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
{% endif %}
#    mountPath: /mnt/disks/disk4
#    device: /dev/nvme3n1
#    fsType: ext4

{% endif %}
##########################
## Device Configuration ##
##########################

# VMRA doest not support fpga yet.
{% if fpga in ['on', 'optional'] and vm_mode != 'on' %}
# Intel FPGA card
configure_fpga: {% if fpga == 'on' %}true{% else %}false{% endif %}

# When configure_fpga is set to true, uncomment below two lines and fit w/ correct values
# fpga_driver_staging_folder: /tmp/intel_fpga/
# fpga_install_script: fpga-ofs-2022-10-06-rc3-deb.sh

{% endif %}

{% if sgx in ['on', 'optional'] and arch in ['icx', 'spr', 'emr', 'gnr'] %}
# Intel Software Guard Extensions (SGX)
configure_sgx: {% if sgx == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if gpu in ['on', 'optional'] %}
# Intel custom GPU kernel - this is required to be true in order to deploy Intel GPU Device Plugin on that node
configure_gpu: {% if gpu == 'on' %}true{% else %}false{% endif %}

{% endif %}
{% if dlb in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] %}
# Configure SIOV and Intel DLB devices - required for Intel DLB Device Plugin support
configure_dlb_devices: {% if dlb == "on" %}true{% else %}false{% endif %}


{% endif %}
{% if dsa in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] %}
# Configure SIOV and Intel DSA devices - required for Intel DSA Device Plugin support
configure_dsa_devices: {% if dsa == "on" %}true{% else %}false{% endif %}


# Example DSA devices configuration list. If left empty and configure_dsa_devices is set to true then default configuration will be applied.
# It is possible to configure more DSA devices by extending dsa_devices list based on example config.
dsa_devices: []
#dsa_devices:
#  - name: dsa0                         # Name of DSA device from /sys/bus/dsa/devices/
#    groups: 1                          # Number of groups to configure. The maximum number of groups per device can be found on /sys/bus/dsa/devices/dsaX/max_groups
#    engines: 1                         # Number of engines to configure - one engine per group will be configured.
#                                       # The maximum number of engines can be found on /sys/bus/dsa/devices/dsa0/max_engines
#    wqs:                               # Work queues will be named as wq<dsa_id>.<wq_id>, for example wq0.0 - WQ with id 0 owned by dsa0 device
#      - id: 0                          # Work queue id
#        mode: "dedicated"              # Supported values: ["shared", "dedicated"]
#        type: "user"                   # Supported values: ["kernel", "user"]
#        size: 8                        # Sum of all configured WQs size must be less than /sys/bus/dsa/devices/dsa0/max_workqueue_size
#        prio: 4                        # Must be set between 1 and 15
#        threshold: 7                   # Only for WQ in mode "shared" - must be at least one less than size of WQ
#        group_id: 0                    # Work queue will be assigned to specific group
#        max_batch_size: 1024           # Specify the max batch size used by a work queue - powers of 2 are accetable
#        max_transfer_size: 2147483648  # Specify the max transfer size used by a work queue - powers of 2 are accetable
#        block_on_fault: 0              # Supported values: [0, 1] - enables (1) or disables (0) block on fault.
#                                       # If a page fault occurs on a source or destination memory access, the operation stops and the page fault is reported to the software

{% endif %}
{% if intel_sriov_fec_operator in ['on', 'optional'] %}
# Wireless FEC H/W Accelerator Device (e.g. ACC100) PCI ID
fec_acc: "0000:6f:00.0"   # must be string in extended Bus:Device.Function (BDF) notation

{% endif %}
{% if qat in ['on', 'optional'] %}
# Enabling this feature will install QAT drivers + services (OOT Drivers), otherwise Intree will be used.
update_qat_drivers: {% if qat == "on" %}true{% else %}false{% endif %}

{% if arch == "emr" and qat == "on" %}
# EMR QAT driver version
nda_qat_driver_package: QAT20.L.1.1.20-00030.tar.gz
# SHA1 sum value for the driver package
nda_qat_driver_pkg_checksum: c578a26b876823174441dfced9ac4fcaa41697ec
# Path to store the EMR QAT package on the ansible host.
nda_qat_driver_folder: /tmp/nda_qat/
{% endif %}

{% if arch == "gnr" and qat == "on" %}
# GNR QAT driver version
#nda_qat_driver_package: QAT21.L.1.2.1-00016.tar.gz
#nda_qat_driver_package: QAT20.L.1.2.21-00014.tar.gz
nda_qat_driver_package: QAT20.L.1.2.20-00064.tar.gz
nda_qat_driver_package_rocky: gnr_sp_alpha_centos_qat21.l.1.2.1-00013.tar.gz
# SHA1 sum value for the driver package
#nda_qat_driver_pkg_checksum: 8edf08b0a7dd76479f0e3af0085790b8b5901baf
#nda_qat_driver_pkg_checksum: 81d4af17cd5e065da2ea91439fbdd271b1488812
nda_qat_driver_pkg_checksum: 854dc526aa3ef85ef1a1e3a3c03b9a83b8363d52
nda_qat_driver_pkg_checksum_rocky: 78d2bd9d2022ee662d33c179c3d876b8e733cd49
# Path to store the GNR QAT package on the ansible host.
nda_qat_driver_folder: /tmp/nda_qat/
{% endif %}

# Enabling the option will configure the QAT device. Must be enabled when qat is on.
configure_qat: {% if qat == "on" %}true{% else %}false{% endif %}

# There are two services on the system which can be used to start qat devices. They can't run in parallel. One of them needs to be enabled and the second one disabled.
enabled_qat_service: "qat"
disabled_qat_service: "qat_service"

{% if arch in ['spr', 'emr', 'gnr'] %}
enable_qat_svm: false         # Enable QAT Shared Virtual Memory (SVM). Only for OOT driver.

{% endif %}
# QAT parameters used by auto detection of qat devices

qat_vf_driver_required: {% if arch in ['spr', 'emr', 'gnr'] %}"4xxxvf"{% else %}"c6xxvf"{% endif %}


# QAT interface configuration list
# Leave empty for auto-configuration of QAT devices.
qat_devices: []
#qat_devices:
{% if on_vms == 'on' %}
#  - qat_id: "0000:0a:00.0"
#    qat_sriov_numvfs: 0                # Has to be set to 0 here to not create any VFs inside VM.

#  - qat_id: "0000:0b:00.0"
#    qat_sriov_numvfs: 0                # Has to be set to 0 here to not create any VFs inside VM.
{% else %}
#  - qat_id: "0000:ab:00.0"             # QAT device id one using DPDK compatible driver for VF devices to be used by vfio-pci kernel driver, replace as required
#    qat_sriov_numvfs: 12               # Number of VFs per PF to create - cannot exceed the maximum number of VFs available for the device. Set to 0 to not create any VFs.
#                                       # Note: Currently when trying to create fewer virtual functions than the maximum, the maximum number always gets created.
#    qat_default_vf_driver: {% if arch in ['spr', 'emr', 'gnr'] %}"4xxxvf"{% else %}"c6xxvf"{% endif %}

#    qat_vfs:                           # Used to configure a non-default VF driver for individual VFs
#      vf_00: "vfio-pci"                # Configures the 1st VF with "vfio-pci" driver
#      vf_05: "vfio-pci"                # Configured the 6th VF with "vfio-pci" driver

#  - qat_id: "0000:xy:00.0"
#    qat_sriov_numvfs: 10
#    qat_default_vf_driver: {% if arch in ['spr', 'emr', 'gnr'] %}"4xxxvf"{% else %}"c6xxvf"{% endif %}

#    qat_vfs: {}                        # To use the default VF driver for all VFs
{% endif %}

{% endif %}

{% if arch in ['spr', 'emr', 'gnr'] and tdx in ['on', 'optional']%}
# EMR TDX configuration
configure_tdx: {% if tdx == 'on' %}true{% else %}false{% endif %}

tdx_version: "1.5"        # only ["1.0", "1.5"] supported
{% endif %}

###############
## Operators ##
###############

{% if intel_ethernet_operator.enabled in ['on', 'optional'] %}
# Intel Ethernet Operator for Intel E810 series ethernet network adapters
intel_ethernet_operator:
{% if intel_ethernet_operator.ddp_update in ['on', 'optional'] %}
  ddp_update: {% if intel_ethernet_operator.ddp_update == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                 # Perform DDP update on PFs listed in dataplane_interfaces using selected DDP profile
{% endif %}
  fw_update: {% if intel_ethernet_operator.fw_update == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                  # Perform firmware update on PFs listed in dataplane_interfaces
  # ClusterFlowConfig does not require additional configuration and can be used in conjunction with NodeFlowConfig
  node_flow_config_enabled: false   # Enable NodeFlowConfig
  # NodeFlowConfig/ClusterFlowConfig manifests local path
  # For more information refer to:
  # https://github.com/intel/intel-ethernet-operator/blob/main/docs/flowconfig-daemon/creating-rules.md
  #flow_config_dir: /tmp/flow_config

{% endif %}
{% if sriov_operator in ['on', 'optional'] %}
# Custom SriovNetworkNodePolicy manifests local path
#custom_sriov_network_policies_dir: /tmp/sriov

{% endif %}
###############################
## Telemetry & Observability ##
###############################

{% if telemetry.collectd in ['on', 'optional'] %}
# Telemetry configuration
# intel_pmu plugin collects information provided by Linux perf interface.
enable_intel_pmu_plugin: false
{% if on_vms == 'on' %}

# Temporary Fix for collectd startup issue on VMs
enable_pkgpower_plugin: false
{% endif %}

# CPU Threads to be monitored by Intel PMU Plugin.
# Please refer to https://collectd.org/wiki/index.php/Plugin:Intel_PMU for configuration details.
intel_pmu_plugin_monitored_cores: ""    # If the field is empty, all available cores will be monitored.

# CPU Threads to be monitored by Intel RDT Plugin.
# Please refer to https://collectd.org/wiki/index.php/Plugin:IntelRDT for configuration details.
intel_rdt_plugin_monitored_cores: ""    # If the field is empty, all available cores will be monitored.

# Additional list of plugins that will be excluded from collectd deployment.
exclude_collectd_plugins: []

{% endif %}
######################
## Power Management ##
######################
{% if power.manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
# The performance profile is available for nodes that has CPU max MHz > 3500.0000 - use 'lscpu' command to see your node details
# To use PowerProfiles in this list as sample pods on this node, please set 'deploy_example_pods' to true in group_vars
power_profiles: [balance-performance] # Possible PowerProfiles are: [performance, balance-performance, balance-power]

{% if power.frequency_scaling in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
frequency_scaling_driver: intel_pstate # Possible values: [intel_pstate, intel_cpufreq]
{% endif %}

# Power Manager Shared Profile/Workload settings.
# It is possible to create node-specific Power Profile
local_shared_profile:
  enabled: false # Enable/Disable local shared profile
  local_max_frequency: 2000
  local_min_frequency: 1500

{% if power.frequency_scaling in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
  # available governors:
  # "powersave" - Lowest frequency within the borders of min_frequency and max_frequency.
  # "performance" - Highest frequency within the borders of min_frequency and max_frequency.
  # "userspace" - !CPUFREQ ONLY! - Allow user space to set CPU frequency in scaling_setspeed attribute
  # "schedutil" - !CPUFREQ ONLY! - Uses data from CPU scheduler to set up frequency
  local_governor: "powersave"
{% endif %}

# Shared Workload is required to make use of Shared Power Profile
shared_workload:
  enabled: {% if name == "on_prem_sw_defined_factory" %}false{% else %}true{% endif %}    # Enable/Disable shared workload
  reserved_cpus: []               # The CPUs in reserved_cpus should match the value of the reserved system CPUs in your Kubelet config file, if none please
                                  # set a dummy core here - the last one to avoid AppQos bug.
  shared_workload_type: "global"  # Set to node name to make use of node-specific Power Profile, 'global' means use cluster-specific custom Power Profile

{% if power.uncore_frequency in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] %}
# EMR uncore_frequency has not supported in the kernel driver yet.
uncore_frequency:
  enabled: {% if power.uncore_frequency == "on" %}true{% else %}false{% endif %} # Enable/Disable uncore frequency

  # The min/max values must be within the range of values supported by the CPU model.
  # Please refer to the documentation of your CPU model to check the supported uncore frequencies.
  system_max_frequency: 2300000
  system_min_frequency: 1300000

  # If needed, you can choose specific frequency per die
  die_selector: []
  # die_selector:
  #   - package: 0
  #     die: 0
  #     min: 1500000
  #     max: 2400000
{% endif %}

{% if power.cstate in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
cstates:
  enabled: {% if power.cstate == "on" %}true{% else %}false{% endif %} # Enable/Disable cstates

  shared:
    C1: true
  profile_exclusive:
    balance-performance:
      C1: false

  # If needed, you can choose specific C-State for each core:
  core: {}
  # core:
  #   "3":
  #     C1: true
  #     C6: false
{% endif %}
{% if power.time_of_day in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
# There is known limitation in upstream, where only one time of day is allowed in cluster.
# This should be fixed in the next release of KPM.
# When using multiple workers, only enable time of day for one of them.
time_of_day:
  enabled: {% if power.time_of_day == "on" %}true{% else %}false{% endif %}

  time_zone: "Europe/Prague" # refer to 'IANA timezone database' for valid value
  schedule: []
    # schedule examples:
    # - time: "14:24"
    #   # powerProfile sets the profile for the shared pool
    #   powerProfile: balance-performance
    #
    #   # this transitions exclusive pods matching a given label from one profile to another
    #   # please ensure that only pods to be used by power manager have this label
    #   pods:
    #     - labels:
    #         matchLabels:
    #           power: "true"
    #       target: balance-performance
    #     - labels:
    #         matchLabels:
    #           special: "false"
    #       target: balance-performance
    #
    #   # cState field simply takes a cstate spec
    #   cState:
    #     sharedPoolCStates:
    #       C1: false
    #       C6: true
    #
    # - time: "14:26"
    #   powerProfile: performance
    #   cState:
    #     sharedPoolCStates:
    #       C1: true
    #       C6: false
    #
    # - time: "14:28"
    #   powerProfile: balance-power
  reserved_cpus: [0, 1]
{% endif %}

{% endif %}
{% if sst in ['on', 'optional'] %}
{% if arch in ['icx', 'spr', 'emr', 'gnr'] %}
# Intel(R) SST-PP (perf-profile) configuration
sst_pp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}

sst_pp_config_list:
  - sst_bf: "enable"            # enable/disable Intel(R) SST-BF (base-freq) configured through SST-PP.
  - sst_cp: "enable"            # enable/disable Intel(R) SST-CP (core-power) configured through SST-PP.
  - sst_tf: "enable"            # enable/disable Intel(R) SST-TF (turbo-freq) configured through SST-PP.
    online_cpus_range: "auto"   # "auto" configures turbo-freq for all available online CPUs.
                                # Alternatively define specific CPUs such as "2,3,5" to prioritize among others.

{% endif %}
{% if arch == 'clx' %}
# Intel Speed Select Base-Frequency configuration.
sst_bf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}

# Intel Speed Select Base-Frequency configuration for Cascade Lake (CLX) Platforms.
# CLX support of SST-BF requires 'intel_pstate' to be 'enabled'
# Option clx_sst_bf_mode requires sst_bf_configuration_enabled to be set to 'true'.
# There are three configuration modes:
# [s] Set SST-BF config (set min/max to 2700/2700 and 2100/2100)
# [m] Set P1 on all cores (set min/max to 2300/2300)
# [r] Revert cores to min/Turbo (set min/max to 800/3900)
clx_sst_bf_mode: s
{% endif %}
{% if arch == 'icx' %}
# Intel Speed Select Base-Frequency configuration for Ice Lake (ICX) Platforms.
# Requires `sst_bf_configuration_enabled` variable to be 'true'
icx_sst_bf_enabled: {% if sst == "on" %}true{% else %}false{% endif %}

# Prioritze (SST-CP) power flow to high frequency cores in case of CPU power constraints.
icx_sst_bf_with_core_priority: {% if sst == "on" %}true{% else %}false{% endif %}


# SST CP config
# Variables are only examples. For more information, please visit:
# https://www.kernel.org/doc/html/latest/admin-guide/pm/intel-speed-select.html#enable-clos-based-prioritization
# Enabling this configuration overrides `icx_sst_bf_with_core_priority`.
sst_cp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}

sst_cp_priority_type: "1"   # For Proportional select "0" and for Ordered select "1", update as required
sst_cp_clos_groups:         # configure up to 4 CLOS groups (id: 0-3)
  - id: 0
    frequency_weight: 0     # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 1
    frequency_weight: 0
    min_MHz: 0
    max_MHz: 25500

sst_cp_cpu_clos:  # Assign required values to CLOS group after priority type setup
  - clos: 0       # Associate CPUs with a CLOS group 0 (id: 0)
    cpus: 2,3,5   # List of CPUs to associate with CLOS group
  - clos: 1
    cpus: 12

# Intel(R) SST-TF (feature turbo-freq) configuration for Ice Lake (ICX) Platforms.
sst_tf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}


{% endif %}
{% endif %}
#######################
## Workloads & Demos ##
#######################

{% if adq_dp in ['on', 'optional'] %}
# Note: ADQ is experimental feature and enabling it may lead to unexpected results.
# ADQ requires back-to-back connection between control plane and worker node on CVL interfaces.
# Name of CVL interfaces must be the same on both nodes, IP address must be present.
# In inventory.ini set "ip=" to IP address of CVL interface
# Additional requirements and details can be found in docs/adq.md
adq_dp:
  enabled: false
  # IP address of CVL interface located on the worker node
  interface_address: "192.168.0.11"
{% endif %}

{% if intel_csl_excat in ['on', 'optional'] and vm_mode != "on" %}
# please mark the node to support excat dp by change the value to true
excat_dp_enabled: false
{% endif %}

{% if intel_eci %}
# Intel ECI (Edge Controls for Industrial)
intel_eci_enabled: {% if intel_eci.enable == "on" %}true{% else %}false{% endif %} # if true, deploy Intel ECI
intel_eci:
  eci-process-automation: {% if intel_eci.process_automation == 'on' %}true{% else %}false{% endif %}

  eci-manufacturing-equipment: {% if intel_eci.manufacturing_equipment == 'on' %}true{% else %}false{% endif %}

  eci-discrete-manufacturing: {% if intel_eci.discrete_manufacturing == 'on' %}true{% else %}false{% endif %}


# The following ECI meta-packages aren't included becuase they are NOT suported yet on Ubuntu (as of 05/2023):
# eci-robotics-control
# eci-robotics
# eci-rth
# eci-kvm
# eci-xenomai

opcua_framework:
  codesys_opcua_client: {% if opcua_framework.codesys_opcua_client == 'on' %}true{% else %}false{% endif %}

  standalone_opcua_server: {% if opcua_framework.standalone_opcua_server == 'on' %}true{% else %}false{% endif %}

{% if intel_eci.enable == 'on' %}

cat_enable: true
# Change this CAT configuration according to your CPU architecture
cat_define: "llc:0=0x0f;llc:1=0xf0"
cat_affinity: "llc:0=0;llc:1=1,3"

ethercat_mac: ""

{% endif %}
{% endif %}

{% if vm_mode in ['on'] and on_vms != 'on' %}
########################
## VMRA Configuration ##
########################

# Only a common VM image for all VMs inside deployment is supported at the moment
{% if secondary_host == 'true' %}
# Secondary vm_host - VM image can only be changed on the first vm_host
# Secondary vm_host - do not change dhcp settings here
dhcp: []

{% else %}
# Default VM image version is Ubuntu 22.04. Uncomment relevant options below to modify.
#vm_image_distribution: "rocky"     # VM image distribution. Supports ['ubuntu, 'rocky']. Default is 'ubuntu'
#vm_image_version_ubuntu: "22.04"   # Ubuntu VM image version. Supoorts ['22.04'].
#vm_image_version_rocky: "9.0"      # Rocky VM image version. Supports ['8.5', '9.0']. Default is '8.5'

# DHCP for VXLAN has to be enabled only on the first vm_host
dhcp:
  - 120

vxlan_gw_ip: "172.31.0.1/24"
{% endif %}
# Set hashed password for root user inside VMs. Current value is just placeholder.
# To create hashed password use e.g.: openssl passwd -6 -salt SaltSalt <your_password>
vm_hashed_passwd: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'

# Set hashed password for non-root user inside VMs. Current value is just placeholder.
# If value is not specified then vm_hashed_passwd value will be used for non root user as well
#vm_hashed_passwd_non_root: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'

# Set physical network subnet, which will be used to create VXLANs for VM communication
# This parameter is required for VM multinode configuration
vxlan_physical_network: "11.0.0.0/8"

#cpu_host_os will change number of CPUs reserved for host OS. Default value is 16
{% if name == "on_prem_sw_defined_factory" %}
cpu_host_os: 2
{% else %}
#cpu_host_os: 8
{% endif %}

# VM cluster name is used to group all VMs from single deployment together
#vm_cluster_name: "cluster1.local"

vms:
{% if secondary_host == 'true' %}
#  - type: "ctrl"       # Type of VM (controller, worker). Supported values: ["ctrl", "work"]
#    name: "vm-ctrl-1"
#    # By default CPU and NUMA node allocation is done automatically.
#    # The 'cpus' and 'numa' options can be used to manually set values.
#    #cpus: "8-11,64-67"
#    #numa: 0
#    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
#    cpu_total: 8
#    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
#    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
#    #alloc_all: true
#    memory: 20480
#    vxlan: 120
{% else %}
  - type: "ctrl"        # Type of VM (controller, worker). Supported values: ["ctrl", "work"]
    name: "vm-ctrl-1"
    # By default CPU and NUMA node allocation is done automatically.
    # The 'cpus' and 'numa' options can be used to manually set values.
    #cpus: "8-11,64-67"
    #numa: 0
    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
    cpu_total: 8
    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
    #alloc_all: true
    memory: 20480
    vxlan: 120
{% endif %}
#  - type: "ctrl"
#    name: "vm-ctrl-2"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
#    name: "vm-ctrl-3"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
  - type: "work"
{% if secondary_host == 'true' %}
    name: "vm-work-2"
{% else %}
    name: "vm-work-1"
{% endif %}
    #cpus: "28-35,84-91"
    #numa: 1
    cpu_total: 16
    #alloc_all: true
    memory: 51200
    vxlan: 120
    # leave empty for auto-configuration of NIC and QAT devices.
    pci: []
{% if name not in ['build_your_own'] %}
    #pci:
    #  - "18:02.2"       # 18:xx.x are example VFs for networking
    #  - "18:02.3"
    #  - "18:02.4"
    #  - "18:02.5"
    nic_devices_count: 4 # Used for auto-configuration of NICs.
{% if qat == "on" %}
    #  - "3d:01.1"       # 3x:xx.x are example VFs for QAT
    #  - "3f:01.1"
    qat_devices_count: 2 # Used for auto-configuration of QAT.
{% endif %}
{% endif %}
#  - type: "work"
{% if secondary_host == 'true' %}
#    name: "vm-work-4"
{% else %}
#    name: "vm-work-3"
{% endif %}
#    cpu_total: 16
#    memory: 51200
#    vxlan: 120
#    # leave empty for auto-configuration of NIC and QAT devices.
#    pci: []
{% if name not in ['build_your_own'] %}
#    #pci:
#    #  - "18:02.0"      # 18:xx.x are example VFs for networking
#    #  - "18:02.1"
#    #  - "18:02.6"
#    #  - "18:02.7"
#    nic_devices_count: 4 # Used for auto-configuration of NICs.
{% if qat == "on" %}
#    #  - "3d:01.2"      # 3x:xx.x are example VFs for QAT
#    #  - "3f:01.2"
#    qat_devices_count: 2 # Used for auto-configuration of QAT.
{% endif %}
{% endif %}
#  - type: "vm"
{% if secondary_host == 'true' %}
#    name: "vm-2"
{% else %}
#    name: "vm-1"
{% endif %}
#    cpu_total: 4
#    memory: 51200
#    vxlan: 120
#    # leave empty for auto-configuration of NIC and QAT devices.
#    pci: []
{% if name not in ['build_your_own'] %}
#    #pci:
#    #  - "18:02.0"      # 18:xx.x are example VFs for networking
#    #  - "18:02.1"
#    #  - "18:02.6"
#    #  - "18:02.7"
#    nic_devices_count: 4 # Used for auto-configuration of NICs.
{% if qat == "on" %}
#    #  - "3d:01.2"      # 3x:xx.x are example VFs for QAT
#    #  - "3f:01.2"
#    qat_devices_count: 2 # Used for auto-configuration of QAT.
{% endif %}
{% endif %}


{% endif %}
