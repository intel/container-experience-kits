---
## Container Experience Kits (CEK) primary playbook variables ##
# Do not change profile_name and configured_arch here!
# To generate vars for different profile/architecture use make command
# At present, the profile and arch generated are as follows
profile_name: {{ name }}
configured_arch: {{ arch }}

# CEK project directory on all nodes
project_root_dir: /opt/cek/

vm_enabled: {% if vm_mode == 'on' %}true{% else %}false{% endif %}
{%- if vm_mode in ['optional'] %}
# vm_mode can't be enabled manually here
# To enable it, vm specific configuration from examples/vm need to be taken
{%- endif %}

# Kubernetes version
kubernetes: true
kube_version: v1.23.4
#kube_version: v1.22.3
#kube_version: v1.21.5
# To deploy only container runtime set this variable as "true", and kubernetes as "false"
# Set both variables as "false" to perform only host configuration
container_runtime_only_deployment: false

# Kubernetes container runtime: docker, containerd, crio
# When "crio" is set, please enable "crio_registries" section
container_runtime: docker

# Preflight will check vars configuration
# It is NOT recommended to disable preflight, unless it is a conscious decision
preflight_enabled: true

# Run system-wide package update (apt dist-upgrade, yum update, ...)
# Note: enabling this may lead to unexpected results
# Tip: you can set this per host using host_vars
update_all_packages: false
update_kernel: false

# Add arbitrary parameters to GRUB
additional_grub_parameters_enabled: false
additional_grub_parameters: ""

# SELinux configuration state: current, enabled, disabled
selinux_state: current
{% if nfd in ['on', 'optional'] %}
# Node Feature Discovery
nfd_enabled: {% if nfd == 'on' %}true{% else %}false{% endif %}
nfd_build_image_locally: false
nfd_namespace: kube-system
nfd_sleep_interval: 60s
{% endif %}

{%- if kube_dashboard in ['on', 'optional'] %}
# Kubernetes Dashboard
kube_dashboard_enabled: {% if kube_dashboard == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if native_cpu_manager in ['on', 'optional'] %}
# Native CPU Manager (Kubernetes built-in)
# Setting this option as "true" enables the "static" policy, otherwise the default "none" policy is used.
# The reserved CPU cores settings are individual per each worker node, and therefore are available to configure in the host_vars file
native_cpu_manager_enabled: {% if native_cpu_manager == 'on' %}true{% else %}false{% endif %}
{% endif %}
{% if topology_manager in ['on', 'optional'] -%}
# Enable Kubernetes built-in Topology Manager
topology_manager_enabled: {% if topology_manager == 'on' %}true{% else %}false{% endif %}
# There are four supported policies: none, best-effort, restricted, single-numa-node.
topology_manager_policy: "best-effort"
{% endif %}

{%- if sriov_operator in ['on', 'optional'] %}
# OpenShift SRIOV Network Operator
sriov_network_operator_enabled: {% if sriov_operator == 'on' %}true{% else %}false{% endif %}
{%- if vm_mode in ['on', 'optional'] %}
# For VM mode sriov_network_operator_enabled has to be false, otherwise VFs
# are not created before VM creation
{%- endif %}
sriov_network_operator_namespace: "sriov-network-operator"
{% endif %}

{%- if sriov_network_dp in ['on', 'optional'] %}
# Intel SRIOV Network Device Plugin
sriov_net_dp_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}
sriov_net_dp_namespace: kube-system
# whether to build and store image locally or use one from public external registry
sriov_net_dp_build_image_locally: false
# SR-IOV network device plugin configuration.
# For more information on supported configuration refer to: https://github.com/intel/sriov-network-device-plugin#configurations
sriovdp_config_data: |
    {
        "resourceList": [{
                "resourceName": "intel_sriov_netdevice",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed", "1889"],
                    "drivers": ["iavf", "ixgbevf"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_700_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed"],
                    "drivers": ["vfio-pci"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_800_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"]
                }
            {% if name in ['full_nfv', 'access', 'regional_dc', 'build_your_own'] -%}
            },
            {
                "resourceName": "intel_fpga",
                "deviceType": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d90"]
                }
            }
            {%- else -%}
            }
            {%- endif %}
        ]
    }
{% endif %}

{%- if power_manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] %}
# Intel Kubernetes Power Manager
intel_power_manager:
  enabled: {% if power_manager == 'on' %}true{% else %}false{% endif %}   # enable intel_power_manager
  # The performance profile is available for nodes that has CPU max MHz > 3500.0000 - use 'lscpu' command to see your node details
  power_profiles: [performance, balance-performance, balance-power]       # the list of PowerProfiles that will be available on the nodes
                                                                          # possible PowerProfiles are: performance, balance_performance, balance_power
  power_nodes: []                                                         # list of nodes that should be considered during Operator work and profiles deployment
    # - node1
    # - node2
  build_image_locally: false
  deploy_example_pods: false                                      # deploy example Pods that will utilize special resources
  global_shared_profile_enabled: false                            # deploy custom Power Profile with user defined frequencies that can be applied to all power nodes
                                                                  # to make use of Shared Profile fill Shared Workload settings in host vars
  max_shared_frequency: 1500                                      # max frequency that will be applied for cores by Shared Workload
  min_shared_frequency: 1000                                      # min frequency that will be applied for cores by Shared Workload
{% endif %}

{%- if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr'] or
    gpu_dp in ['on', 'optional'] or
    qat_dp in ['on', 'optional'] %}
# Intel Device Plugin Operator
intel_dp_namespace: kube-system # namespace will be applied for SGX DP, GPU DP and QAT DP
{% endif %}

{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
# Intel Ethernet Operator for Intel E810 Series network interface cards
intel_ethernet_operator_enabled: {% if intel_ethernet_operator.enabled == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}
intel_ethernet_operator_flow_config_enabled: {% if intel_ethernet_operator.flow_config == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}
{% endif %}

{%- if intel_sriov_fec_operator in ['on', 'optional'] %}
# Intel Operator for SR-IOV Wireless Forward Error Correction (FEC) Accelerators
intel_sriov_fec_operator_enabled: {% if intel_sriov_fec_operator == 'on' %}true{% else %}false{% endif %} # if true, deploy FEC Operator
{% endif %}

{%- if qat_dp in ['on', 'optional'] %}
# Intel QAT Device Plugin for Kubernetes
qat_dp_enabled: {% if qat_dp == 'on' %}true{% else %}false{% endif %}
qat_dp_verbosity: 4
# Maximum number of QAT devices (minimum is 1) to be provided to the QAT Device Plugin.
# To use all available QAT devices on each node, qat_dp_max_devices must be equal to the highest number of QAT Devices from all nodes
# e.g node1 - 48VFs, node2 - 32VFs, qat_dp_max_devices: 48
# It is possible to use a subset of QAT devices in QAT DP. E.g by putting 10 here, QAT DP will use just 10VFs on each node
qat_dp_max_num_devices: 32
qat_dp_build_image_locally: {% if vm_mode == 'on' %}true{% else %}false{% endif %}

qat_supported_pf_dev_ids:
  - "435"
  - "37c8"
  - "19e2"
  - "18ee"
  - "6f54"
  - "18a0"
  - "4940"
  - "4942"

qat_supported_vf_dev_ids:
  - "443"
  - "37c9"
  - "19e3"
  - "18ef"
  - "6f55"
  - "18a1"
  - "4941"
  - "4943"
{% endif %}

{%- if openssl in ['on', 'optional'] %}
# This feature will enable OpenSSL*Engine
openssl_engine_enabled: {% if openssl == 'on' and qat == 'on' %}true{% else %}false{% endif %} # to activate OpenSSL*Engine set both install_openssl and update_qat_drivers to 'true' in host_vars
{% endif %}

{%- if gpu_dp in ['on', 'optional'] %}
# Intel GPU Device Plugin for Kubernetes
gpu_dp_enabled: {% if gpu == 'on' %}true{% else %}false{% endif %}
gpu_dp_kernel_version: "5.4.48+"
gpu_dp_verbosity: 4
gpu_dp_build_image_locally: false

# Configuration-options
# To fully discover the below settings usage, please refer to: https://github.com/intel/intel-device-plugins-for-kubernetes/tree/v0.23.0/cmd/gpu_plugin
gpu_dp_shared_devices: 10  # number of containers (min. 1) that can share the same GPU device
gpu_dp_monitor_resources: false  # enable monitoring all GPU resources on the node
gpu_dp_fractional_manager: false  # enable handling of fractional resources for multi-GPU nodes
gpu_dp_prefered_allocation: 'none'  # available policies are: ['balanced', 'packed', 'none']

# For systems with older kernel drivers or GPUs which do not support reading the GPU memory amount,
# the gpu_dp_max_memory variable value is turned into a GPU memory amount label instead of a read value.
# Please set carefully as this will be set as the maximum available memory of GPU on the nodes!
# If GPU will be capable of manifest its size via the driver this variable will be ignored.
gpu_dp_max_memory: "8 GB" # max memory per card - for Intel SG1 single card has 8 GB of memory
{% endif %}

{%- if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr'] %}
# Intel SGX Device Plugin for Kubernetes
sgx_dp_enabled: {% if sgx_dp == 'on' %}true{% else %}false{% endif %}
sgx_dp_verbosity: 4
sgx_dp_build_image_locally: false
sgx_aesmd_namespace: intel-sgx-aesmd
# Deploy SGX AESMD demo app workload
sgx_aesmd_demo_enable: false
# ProvisionLimit is a number of containers that can share the same SGX provision device.
sgx_dp_provision_limit: 20
# EnclaveLimit is a number of containers that can share the same SGX enclave device.
sgx_dp_enclave_limit: 20
{% endif %}

{%- if (kmra and (kmra.pccs in ['on', 'optional'] or
    kmra.apphsm in ['on', 'optional'] or
    kmra.ctk_demo in ['on', 'optional'])) and
    arch in ['icx', 'spr'] %}
# KMRA (Key Management Reference Application)
kmra:
  {%- if kmra.pccs in ['on', 'optional'] %}
  pccs:
    enabled: {% if kmra.pccs == 'on' %}true{% else %}false{% endif %}   # enable PCCS application
    # The PCCS uses this API key to request collaterals from Intel's Provisioning Certificate Service. User needs to subscribe first to obtain an API key.
    # For how to subscribe to Intel Provisioning Certificate Service and receive an API key, go to https://api.portal.trustedservices.intel.com/provisioning-certification,
    # and get an API key by clicking 'Subscribe'.
    api_key: "ffffffffffffffffffffffffffffffff"
  {%- endif %}
  {%- if kmra.apphsm in ['on', 'optional'] %}
  apphsm:
    enabled: {% if kmra.apphsm == 'on' %}true{% else %}false{% endif %}   # enable AppHSM application
  {%- endif %}
  {%- if kmra.ctk_demo in ['on', 'optional'] %}
  ctk_loadkey_demo:
    enabled: {% if kmra.ctk_demo == 'on' %}true{% else %}false{% endif %}   # enable CTK demo application
  {%- endif %}
{% endif %}

{%- if service_mesh and service_mesh.enabled in ['on', 'optional'] %}
# Service mesh deployment
# https://istio.io/latest/docs/setup/install/istioctl/

# for all available options, please, refer to the 'roles/service_mesh_install/vars/main.yml;
service_mesh:
  enabled: {% if service_mesh.enabled == 'on' %}true{% else %}false{% endif %}   # enable Service Mesh
  # available profiles are: 'default', 'demo', 'minimal', 'external', 'empty', 'preview'
  # if custom profile needs to be deployed, please, place the file named '<profile_name>.yaml'
  # into the directory 'roles/service_mesh_install/files/profiles/'
  # 'custom-ca' profile name is reserved for usage by sgx_signer if sgx_signer option is enabled
  # any name provided will be overwritten in this case
  profile: {% if service_mesh.sgx_signer == 'on' and arch in ['icx', 'spr'] %}custom-ca{% else %}default{% endif %}
  {%- if service_mesh.tcpip_bypass_ebpf in ['on', 'optional'] %}
  tcpip_bypass_ebpf:
    enabled: {% if service_mesh.tcpip_bypass_ebpf == 'on' %}true{% else %}false{% endif %}  # enable tcp/ip ebpf bypass demo
  {%- endif %}
  {%- if service_mesh.tls_splicing in ['on', 'optional'] %}
  tls_splicing:
    enabled: {% if service_mesh.tls_splicing == 'on' %}true{% else %}false{% endif %}   # enable TLS splicing demo
  {%- endif %}
  {%- if service_mesh.sgx_signer in ['on', 'optional'] and arch in ['icx', 'spr'] %}
  sgx_signer:
    enabled: {% if service_mesh.sgx_signer == 'on' %}true{% else %}false{% endif %}   # enable automated key management integration
    name: sgx-signer
  {%- endif %}
{% endif %}

{%- if tcs in ['on', 'optional'] and
    arch in ['icx', 'spr'] %}
# Trusted Certificate Service deployment
# https://github.com/intel/trusted-certificate-issuer
tcs:
  enabled: {% if tcs == 'on' %}true{% else %}false{% endif %}   # enable Trusted Certificate Issuer
  build_image_locally: false
{% endif %}

{%- if tca in ['on', 'optional'] and
    arch in ['icx', 'spr'] %}
# Trusted Certificate Attestation controller deployment
# https://github.com/intel/trusted-certificate-attestation
tca:
  enabled: {% if tca == 'on' %}true{% else %}false{% endif %}   # enable Trusted Certificate Attestation Controller
  build_image_locally: false
{% endif %}

{%- if tas in ['on', 'optional'] or gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling (PAS)
pas_namespace: kube-system

{%- if tas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - Telemetry Aware Scheduling (TAS)
tas_enabled: {% if tas == 'on' %}true{% else %}false{% endif %}
tas_build_image_locally: false
# create and enable TAS demonstration policy: [true, false]
tas_enable_demo_policy: false
{% endif %}

{%- if gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - GPU Aware Scheduling (GAS)
gas_enabled: {% if gas == 'on' %}true{% else %}false{% endif %}
gas_build_image_locally: false
{%- endif %}
{%- endif %}

# Telemetry configuration. Collectd and Telegraf variables are mutually exclusive.
{%- if telemetry.prometheus in  ['on', 'optional'] %}
prometheus_operator: {% if telemetry.prometheus == 'on'%}true{% else %}false{% endif %}
{%- endif %}
{%- if telemetry.collectd in ['on', 'optional'] %}
collectd_enabled: {% if telemetry.collectd == 'on'%}true{% else %}false{% endif %}
{%- endif %}
{%- if telemetry.telegraf in ['on', 'optional'] %}
telegraf_enabled: {% if telemetry.telegraf == 'on'%}true{% else %}false{% endif %}
{%- endif %}
collectd_scrap_interval: 30
telegraf_scrap_interval: 30

{% if sriov_network_dp in ["on", "optional"] or network_userspace in ["on", "optional"] -%}
# Create reference net-attach-def objects
example_net_attach_defs:
{%- if sriov_network_dp in ["on", "optional"] %}
  sriov_net_dp: {% if sriov_network_dp == "on" %}true{% else %}false{% endif %} # update to match host_vars CNI configuration
{%- endif -%}
{%- if network_userspace in ["on", "optional"] %}
  userspace_ovs_dpdk: {% if network_userspace == "on" %}true{% else %}false{% endif %} # update to match host_vars CNI configuration
  userspace_vpp: false # update to match host_vars CNI configuration
{%- endif %}
{%- endif %}
{% if firewall in ['on', 'optional'] %}
firewall_enabled: {% if firewall == "on" %}true{% else %}false{% endif %}
{%- endif %}

## Proxy configuration ##
#http_proxy: "http://proxy.example.com:1080"
#https_proxy: "http://proxy.example.com:1080"
#additional_no_proxy: ".example.com,mirror_ip"  #no need to include the following (will be added automatically): localhost, 127.0.0.1, controllerIP, nodesIPs

# (Ubuntu only) disables DNS stub listener which may cause issues on Ubuntu
dns_disable_stub_listener: true

# Kubernetes cluster name, also will be used as DNS domain
cluster_name: cluster.local

## Kubespray variables ##

# Supported network plugins(calico, flannel) and kube-proxy configuration
kube_controller_manager_bind_address: 127.0.0.1
kube_proxy_metrics_bind_address: 127.0.0.1
# supported network plugins(calico, flannel) and kube-proxy configuration
kube_network_plugin: calico
# Supported calico backend: [vxlan, bird]
{%- if vm_mode in ['on'] %}
calico_network_backend: vxlan
# For VM mode calico_backend has to be vxlan, otherwise deployment will fail
{%- else %}
calico_network_backend: bird
{%- endif %}
# Advanced calico options
# https://github.com/kubernetes-sigs/kubespray/blob/master/docs/calico.md
# if set to 'true', variables defined by user will be used and default CEK configuration will be ignored
calico_advanced_options: false
wireguard_enabled: {% if wireguard == 'on' %}true{% else %}false{% endif %}
kube_network_plugin_multus: {% if multus == 'on' %}true{% else %}false{% endif %}
kube_pods_subnet: 10.244.0.0/16
{%- if name in ['regional_dc', 'full_nfv', 'access', 'storage', 'build_your_own'] -%}
{% set mask = 18 %}
{%- elif name == 'remote_fp' -%}
{% set mask = 19 %}
{%- elif name == 'on_prem' -%}
{% set mask = 21 %}
{%- elif name == 'basic' -%}
{% set mask = 22 %}
{%- endif %}
kube_service_addresses: 10.233.0.0/{{ mask }}
kube_proxy_mode: iptables

# Set on true if you want to enable the eBPF dataplane support
calico_bpf_enabled: false

# Set this var to true if you want to expose calico metrics endpoint
calico_metrics_enabled: false

# Comment this line out if you want to expose k8s services of type nodePort externally.
kube_proxy_nodeport_addresses_cidr: 127.0.0.0/8

# Local Docker Hub mirror, if it exists
#docker_registry_mirrors:
#  - http://mirror_ip:mirror_port
#docker_insecure_registries:
#  - http://docker_insecure_registry_ip
#containerd_registries:
#  "docker.io":
#    - "https://registry-1.docker.io"
#    - "http://mirror_ip:mirror_port"
#crio_registries:
#  - prefix: docker.io
#    insecure: false
#    blocked: false
#    location: registry-1.docker.io
#    unqualified: true
#    mirrors:
#      - location: mirror_ip:mirror_port
#        insecure: false
#crio_insecure_registries:
#  - http://crio_insecure_registry_ip

{%- if registry in ['on', 'optional'] %}

# Docker registry running on the cluster allows us to store images not available on Docker Hub
# The range of valid ports is 30000-32767
registry_enable: {% if registry == 'on' %}true{% else %}false{% endif %}
registry_nodeport: 30500
registry_local_address: "localhost:{{ '{{' }} registry_nodeport {{ '}}' }}"
{%- endif %}

{%- if cert_manager in ['on', 'optional'] %}
cert_manager_enable: {% if cert_manager == 'on' %}true{% else %}false{% endif %}
{%- endif %}

# Enable Pod Security Policy. This option enables PSP admission controller and creates minimal set of rules.
psp_enabled: {% if psp == 'on' %}true{% else %}false{% endif %}

# Set image pull policy to Always. Pull images prior to starting containers. Valid credentials must be configured.
always_pull_enabled: false

{%- if minio in ['on', 'optional'] %}

## MinIO variables ##
# Enable Minio Storage service.
minio_enabled: {% if minio == 'on' %}true{% else %}false{% endif %}
minio_tenant_enabled: true                            # Specifies whether to install MinIO Sample Tenant
minio_tenant_servers: 4                               # The number of MinIO Tenant nodes
minio_tenant_volumes_per_server: 4                    # The number of volumes per servers
minio_deploy_test_mode: true                          # true (Test Mode) - use a file as loop device when creating storage
                                                      # called "virtual block device" which is useful for test or automation purpose
                                                      # false (Performance Mode) - use an actual NVME or SSD device when creating storage
{%- endif %}
{%- if cndp in ['on', 'optional'] or cndp_dp in ['on', 'optional'] %}

# Intel Cloud Native Data Plane.
{%- if cndp_dp in ['on', 'optional'] %}
cndp_dp_enabled: {% if cndp_dp == 'on' %}true{% else %}false{% endif %}
{% if cndp_dp == 'on' %}
cndp_net_attach_def_enabled: true         # Whether or not to create NetworkAttachmentDefinition resource.
cndp_net_attach_def_conf:
  name: cndp-cni-afxdp0              # (Optional) Name of NetworkAttachmentDefinition resource.
  ipam:                              # (Optional) ipam configuration section NetworkAttachmentDefinition resource.
    subnet: "192.168.1.0/24"         # (Optional) Default is "192.168.1.0/24".
    rangeStart: "192.168.1.200"      # (Optional) Default is "192.168.1.200".
    rangeEnd: "192.168.1.220"        # (Optional) Default is "192.168.1.220".
    gateway: "192.168.1.1"           # (Optional) Default is "192.168.1.1".
{% else %}
cndp_net_attach_def_enabled: false
{%- endif %}
{%- endif %}
{%- endif %}
